{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cid ='d888370f9c2a4514aade84a1b6ae976e'\n",
    "secret ='01774fdc665e4dcc8dc8c48667e46bf1'\n",
    "\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id=cid, client_secret=secret)\n",
    "sp = spotipy.Spotify(client_credentials_manager = client_credentials_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import exceptions\n",
    "import lyricsgenius as lg\n",
    "import re\n",
    "import requests\n",
    "\n",
    "token = '0GGXQLMawu2kxxYNqTGFT79I0wq8McYb8jbOz0HGOQc-sCAmu0HuYKo_C71dsRg-'\n",
    "genius = lg.Genius(token, retries=3)\n",
    "genius.verbose = True\n",
    "# Remove section headers (e.g. [Chorus]) from lyrics when searching\n",
    "genius.remove_section_headers = True\n",
    "\n",
    "# Include hits thought to be non-songs (e.g. track lists)\n",
    "genius.skip_non_songs = True\n",
    "\n",
    "# Exclude songs with these words in their title\n",
    "genius.excluded_terms = [\"(Remix)\", \"(Live)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'danceability': 0.472, 'energy': 0.746, 'key': 11, 'loudness': -4.816, 'mode': 1, 'speechiness': 0.0321, 'acousticness': 0.00581, 'instrumentalness': 0.663, 'liveness': 0.0999, 'valence': 0.403, 'tempo': 97.44, 'type': 'audio_features', 'id': '7eX5SypK35V8Y9d9pS6rWy', 'uri': 'spotify:track:7eX5SypK35V8Y9d9pS6rWy', 'track_href': 'https://api.spotify.com/v1/tracks/7eX5SypK35V8Y9d9pS6rWy', 'analysis_url': 'https://api.spotify.com/v1/audio-analysis/7eX5SypK35V8Y9d9pS6rWy', 'duration_ms': 162040, 'time_signature': 4}, {'danceability': 0.63, 'energy': 0.908, 'key': 11, 'loudness': -2.42, 'mode': 1, 'speechiness': 0.0326, 'acousticness': 0.0238, 'instrumentalness': 0.592, 'liveness': 0.116, 'valence': 0.968, 'tempo': 120.522, 'type': 'audio_features', 'id': '4fPBB44eDH71YohayI4eKV', 'uri': 'spotify:track:4fPBB44eDH71YohayI4eKV', 'track_href': 'https://api.spotify.com/v1/tracks/4fPBB44eDH71YohayI4eKV', 'analysis_url': 'https://api.spotify.com/v1/audio-analysis/4fPBB44eDH71YohayI4eKV', 'duration_ms': 189227, 'time_signature': 4}]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(\u001b[39m3407\u001b[39m)\n\u001b[0;32m     10\u001b[0m R \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, (\u001b[39m25\u001b[39m, \u001b[39m384\u001b[39m))\n\u001b[1;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msentence_transformers\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m     13\u001b[0m embedder \u001b[39m=\u001b[39m SentenceTransformer(\u001b[39m'\u001b[39m\u001b[39mmsmarco-MiniLM-L-6-v3\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\.conda\\envs\\tf\\lib\\site-packages\\sentence_transformers\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m2.2.2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m __MODEL_HUB_ORGANIZATION__ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msentence-transformers\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m SentencesDataset, ParallelSentencesDataset\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mLoggingHandler\u001b[39;00m \u001b[39mimport\u001b[39;00m LoggingHandler\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mSentenceTransformer\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceTransformer\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\.conda\\envs\\tf\\lib\\site-packages\\sentence_transformers\\datasets\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mDenoisingAutoEncoderDataset\u001b[39;00m \u001b[39mimport\u001b[39;00m DenoisingAutoEncoderDataset\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mNoDuplicatesDataLoader\u001b[39;00m \u001b[39mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mParallelSentencesDataset\u001b[39;00m \u001b[39mimport\u001b[39;00m ParallelSentencesDataset\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mSentencesDataset\u001b[39;00m \u001b[39mimport\u001b[39;00m SentencesDataset\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mSentenceLabelDataset\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceLabelDataset\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\.conda\\envs\\tf\\lib\\site-packages\\sentence_transformers\\datasets\\ParallelSentencesDataset.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgzip\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreaders\u001b[39;00m \u001b[39mimport\u001b[39;00m InputExample\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m List\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\.conda\\envs\\tf\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mimport\u001b[39;00m ndarray\n\u001b[1;32m---> 11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m \u001b[39mimport\u001b[39;00m HfApi, HfFolder, Repository, hf_hub_url, cached_download\n\u001b[0;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\.conda\\envs\\tf\\lib\\site-packages\\transformers\\__init__.py:30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     29\u001b[0m \u001b[39m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     33\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     logging,\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     48\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mget_logger(\u001b[39m__name__\u001b[39m)  \u001b[39m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\.conda\\envs\\tf\\lib\\site-packages\\transformers\\dependency_versions_check.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdependency_versions_table\u001b[39;00m \u001b[39mimport\u001b[39;00m deps\n\u001b[1;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversions\u001b[39;00m \u001b[39mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     20\u001b[0m \u001b[39m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m# order specific notes:\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     26\u001b[0m pkgs_to_check_at_runtime \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpython tqdm regex requests packaging filelock numpy tokenizers\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39msplit()\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\.conda\\envs\\tf\\lib\\site-packages\\transformers\\utils\\__init__.py:34\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconstants\u001b[39;00m \u001b[39mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdoc\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     add_code_sample_docstrings,\n\u001b[0;32m     28\u001b[0m     add_end_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     replace_return_docstrings,\n\u001b[0;32m     33\u001b[0m )\n\u001b[1;32m---> 34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgeneric\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     ContextManagers,\n\u001b[0;32m     36\u001b[0m     ExplicitEnum,\n\u001b[0;32m     37\u001b[0m     ModelOutput,\n\u001b[0;32m     38\u001b[0m     PaddingStrategy,\n\u001b[0;32m     39\u001b[0m     TensorType,\n\u001b[0;32m     40\u001b[0m     cached_property,\n\u001b[0;32m     41\u001b[0m     can_return_loss,\n\u001b[0;32m     42\u001b[0m     expand_dims,\n\u001b[0;32m     43\u001b[0m     find_labels,\n\u001b[0;32m     44\u001b[0m     flatten_dict,\n\u001b[0;32m     45\u001b[0m     is_jax_tensor,\n\u001b[0;32m     46\u001b[0m     is_numpy_array,\n\u001b[0;32m     47\u001b[0m     is_tensor,\n\u001b[0;32m     48\u001b[0m     is_tf_tensor,\n\u001b[0;32m     49\u001b[0m     is_torch_device,\n\u001b[0;32m     50\u001b[0m     is_torch_tensor,\n\u001b[0;32m     51\u001b[0m     reshape,\n\u001b[0;32m     52\u001b[0m     squeeze,\n\u001b[0;32m     53\u001b[0m     tensor_size,\n\u001b[0;32m     54\u001b[0m     to_numpy,\n\u001b[0;32m     55\u001b[0m     to_py_obj,\n\u001b[0;32m     56\u001b[0m     transpose,\n\u001b[0;32m     57\u001b[0m     working_or_temp_dir,\n\u001b[0;32m     58\u001b[0m )\n\u001b[0;32m     59\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mhub\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     60\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[0;32m     61\u001b[0m     DISABLE_TELEMETRY,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m     send_example_telemetry,\n\u001b[0;32m     88\u001b[0m )\n\u001b[0;32m     89\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mimport_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     90\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[0;32m     91\u001b[0m     ENV_VARS_TRUE_VALUES,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m     torch_version,\n\u001b[0;32m    167\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\.conda\\envs\\tf\\lib\\site-packages\\transformers\\utils\\generic.py:33\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mimport_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m is_flax_available, is_tf_available, is_torch_available, is_torch_fx_proxy\n\u001b[0;32m     32\u001b[0m \u001b[39mif\u001b[39;00m is_tf_available():\n\u001b[1;32m---> 33\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m is_flax_available():\n\u001b[0;32m     36\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mjnp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\__init__.py:45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m     44\u001b[0m \u001b[39m# from tensorflow.python import keras\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_column\u001b[39;00m \u001b[39mimport\u001b[39;00m feature_column_lib \u001b[39mas\u001b[39;00m feature_column\n\u001b[0;32m     46\u001b[0m \u001b[39m# from tensorflow.python.layers import layers\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodule\u001b[39;00m \u001b[39mimport\u001b[39;00m module\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_lib.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"FeatureColumns: tools for ingesting and representing features.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# pylint: disable=unused-import,line-too-long,wildcard-import,g-bad-import-order\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_column\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_column\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_column\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_column_v2\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_column\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequence_feature_column\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py:143\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m sparse_tensor \u001b[39mas\u001b[39;00m sparse_tensor_lib\n\u001b[0;32m    142\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_shape\n\u001b[1;32m--> 143\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m base\n\u001b[0;32m    144\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m array_ops\n\u001b[0;32m    145\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m check_ops\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\layers\\base.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# =============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Contains the base Layer class, from which all layers inherit.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlegacy_tf_layers\u001b[39;00m \u001b[39mimport\u001b[39;00m base\n\u001b[0;32m     18\u001b[0m InputSpec \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mInputSpec\n\u001b[0;32m     20\u001b[0m keras_style_scope \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mkeras_style_scope\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m     24\u001b[0m \u001b[39m# See b/110718070#comment18 for more details about this import.\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Input\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\models.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39m\"\"\"Code for model cloning, plus model-related API entries.\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m ops\n\u001b[1;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m metrics \u001b[39mas\u001b[39;00m metrics_module\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m optimizer_v1\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m config_pb2\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m tf2\n\u001b[1;32m---> 34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcheckpoint\u001b[39;00m \u001b[39mimport\u001b[39;00m checkpoint \u001b[39mas\u001b[39;00m tracking_util\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m \u001b[39mimport\u001b[39;00m session \u001b[39mas\u001b[39;00m session_module\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m distribution_strategy_context\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\checkpoint\\checkpoint.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcheckpoint\u001b[39;00m \u001b[39mimport\u001b[39;00m checkpoint_options\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcheckpoint\u001b[39;00m \u001b[39mimport\u001b[39;00m functional_saver\n\u001b[1;32m---> 32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcheckpoint\u001b[39;00m \u001b[39mimport\u001b[39;00m graph_view \u001b[39mas\u001b[39;00m graph_view_lib\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcheckpoint\u001b[39;00m \u001b[39mimport\u001b[39;00m restore \u001b[39mas\u001b[39;00m restore_lib\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcheckpoint\u001b[39;00m \u001b[39mimport\u001b[39;00m util\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\checkpoint\\graph_view.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mweakref\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcheckpoint\u001b[39;00m \u001b[39mimport\u001b[39;00m trackable_view\n\u001b[1;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcheckpoint\u001b[39;00m \u001b[39mimport\u001b[39;00m util\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrackable\u001b[39;00m \u001b[39mimport\u001b[39;00m base\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtf_export\u001b[39;00m \u001b[39mimport\u001b[39;00m tf_export\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\checkpoint\\util.py:30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrackable\u001b[39;00m \u001b[39mimport\u001b[39;00m python_state\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrackable\u001b[39;00m \u001b[39mimport\u001b[39;00m trackable_utils\n\u001b[1;32m---> 30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m optimizer \u001b[39mas\u001b[39;00m optimizer_v1\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m \u001b[39mimport\u001b[39;00m saveable_object \u001b[39mas\u001b[39;00m saveable_object_lib\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m \u001b[39mimport\u001b[39;00m saveable_object_util\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py:23\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mabc\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msix\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute_lib\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute_utils\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m distribution_strategy_context \u001b[39mas\u001b[39;00m distribute_ctx\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:225\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m summary_ops_v2\n\u001b[0;32m    224\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m variable_scope\n\u001b[1;32m--> 225\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlosses\u001b[39;00m \u001b[39mimport\u001b[39;00m losses_impl\n\u001b[0;32m    226\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplatform\u001b[39;00m \u001b[39mimport\u001b[39;00m tf_logging\n\u001b[0;32m    227\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrackable\u001b[39;00m \u001b[39mimport\u001b[39;00m base \u001b[39mas\u001b[39;00m trackable\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m control_flow_ops\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m math_ops\n\u001b[1;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m nn_ops\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m weights_broadcast_ops\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:982\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:925\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1423\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1395\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1555\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:156\u001b[0m, in \u001b[0;36m_path_isfile\u001b[1;34m(path)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:148\u001b[0m, in \u001b[0;36m_path_is_mode_type\u001b[1;34m(path, mode)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:142\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "l = ['7eX5SypK35V8Y9d9pS6rWy',\n",
    "        '4fPBB44eDH71YohayI4eKV',\n",
    "        #'2M7UdnD0fEaryh8TnCvqFX',\n",
    "        #'6LqZFfv4fUP7va14Y6VW9a',\n",
    "]\n",
    "\n",
    "print(sp.audio_features(tracks=l))\n",
    "\n",
    "np.random.seed(3407)\n",
    "R = np.random.normal(0, 1, (25, 384))\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('msmarco-MiniLM-L-6-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_remover = re.compile(r'\\D*')\n",
    "\n",
    "def remove_song_anatomy_tags(l):\n",
    "    l = l.split('\\n')\n",
    "    l = l[1::]\n",
    "    edited = []\n",
    "\n",
    "    last_line = l[-1]\n",
    "    last_line = last_line.split(' ')\n",
    "    last_line[-1] = re.findall(embed_remover, last_line[-1])[0]\n",
    "    l[-1] = ' '.join(last_line)\n",
    "\n",
    "    for i in l:\n",
    "        if i == '':\n",
    "            continue\n",
    "\n",
    "        if i == ' ':\n",
    "            continue\n",
    "\n",
    "        edited.append(i)\n",
    "\n",
    "    return ' '.join(edited)\n",
    "\n",
    "\n",
    "def get_song_lyrics_and_features(track_id, track_name, artist_name, popularity=50, explicit=0):\n",
    "    feature_list = []\n",
    "    track_features = sp.audio_features(tracks = [track_id])[0]\n",
    "\n",
    "    feature_names = ['danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature', 'duration_ms']\n",
    "\n",
    "    for feature in feature_names:\n",
    "        feature_list.append(track_features[feature])\n",
    "        \n",
    "    feature_list.append(popularity)\n",
    "    feature_list.append(explicit)\n",
    "    print(feature_list)\n",
    "\n",
    "    \n",
    "    max_tries = 4\n",
    "\n",
    "    for i in range(max_tries):\n",
    "\n",
    "        try:\n",
    "            song = genius.search_song(track_name, artist_name)\n",
    "            break\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f'{track_name} pooped itself: {i} tries')\n",
    "            if i == max_tries - 1:\n",
    "                song = 'blank'\n",
    "                raise\n",
    "\n",
    "    print(song.lyrics)\n",
    "    embeddings = embedder.encode(remove_song_anatomy_tags(song.lyrics), show_progress_bar=True)\n",
    "    embeddings_proj = np.dot(R, embeddings)\n",
    "\n",
    "    feature_list.extend(embeddings_proj)\n",
    "    print(feature_list)\n",
    "    return feature_list\n",
    "\n",
    "\n",
    "# objs = list(bucket.objects.filter(Prefix=track_id))\n",
    "#     if len(objs) > 0 and objs[0].key == track_id:\n",
    "#         print(f\"{track_name} is already in the S3 bucket.\")\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't Let the Green Grass Fool You Wilson Pickett\n",
      "[0.573, 0.707, 6, -8.29, 0, 0.0632, 0.442, 0.000241, 0.123, 0.961, 152.467, 4, 169707, 51, 0]\n",
      "Searching for \"Don't Let the Green Grass Fool You\" by Wilson Pickett...\n",
      "Done.\n",
      "5 ContributorsDon’t Let the Green Grass Fool You Lyrics\n",
      "Girl, try to remember\n",
      "When we didn't have no shoes\n",
      "We stuck together, just me and you\n",
      "It took a long time to get what we got today\n",
      "Now you wanna give it all up for another guy\n",
      "\n",
      "Baby, I'm tellin' you\n",
      "Don't let the green grass fool ya\n",
      "Don't let it change your mind\n",
      "\n",
      "It's gonna be hard for me, baby\n",
      "To build my life all over\n",
      "Because baby, you got the love\n",
      "That I really need, really\n",
      "'Cause if you leave\n",
      "Leave me this way, baby\n",
      "You know what?\n",
      "I may not live to see a brand new day\n",
      "\n",
      "Oh baby, don't let the green grass fool ya\n",
      "Don't let it change your mind\n",
      "You know why?\n",
      "It may be greener on the other side, baby\n",
      "But right in my arms is where you belong\n",
      "You might also like\n",
      "Mmm, ooh baby, oh, oh, ah\n",
      "\n",
      "Don't let the green grass fool ya\n",
      "Don't let it change your mind, oh baby\n",
      "It may be greener on the other side, baby\n",
      "But ah, right in my arms is where you belong, baby\n",
      "I got, gotta feel it\n",
      "Don't let the green grass fool ya, baby\n",
      "(Don't let it change your mind)\n",
      "Ah, hey (Greener on the other side)\n",
      "Don't, don't let it fool ya\n",
      "Baby, in my arms is where you belong, yeah1Embed\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'embedder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m explicit \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(track[\u001b[39m'\u001b[39m\u001b[39mexplicit\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(track_name, artist_name)\n\u001b[1;32m---> 12\u001b[0m get_song_lyrics_and_features(track_id, track_name, artist_name, popularity, explicit)\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(track_id)\n",
      "Cell \u001b[1;32mIn[5], line 53\u001b[0m, in \u001b[0;36mget_song_lyrics_and_features\u001b[1;34m(track_id, track_name, artist_name, popularity, explicit)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[39mprint\u001b[39m(song\u001b[39m.\u001b[39mlyrics)\n\u001b[1;32m---> 53\u001b[0m embeddings \u001b[39m=\u001b[39m embedder\u001b[39m.\u001b[39mencode(remove_song_anatomy_tags(song\u001b[39m.\u001b[39mlyrics), show_progress_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     54\u001b[0m embeddings_proj \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(R, embeddings)\n\u001b[0;32m     56\u001b[0m feature_list\u001b[39m.\u001b[39mextend(embeddings_proj)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embedder' is not defined"
     ]
    }
   ],
   "source": [
    "playlist_id = 'https://open.spotify.com/playlist/2AXUGe93UP8oVj3F0a1Hoo?si=836ee38e473943a6'\n",
    "results = sp.playlist_tracks(playlist_id)\n",
    "\n",
    "# print(results)\n",
    "for item in results['items'][:3]:\n",
    "    track = item['track']\n",
    "    track_id = track['id']\n",
    "    track_name = track['name']\n",
    "    artist_name = track['artists'][0]['name']\n",
    "    popularity = track['popularity']\n",
    "    explicit = int(track['explicit'])\n",
    "    print(track_name, artist_name)\n",
    "    \n",
    "    # get_song_lyrics_and_features(track_id, track_name, artist_name, popularity, explicit)\n",
    "    print(track_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.34150514 -0.83548739  0.96262807 ...  1.51351945  1.48396066\n",
      "   1.7828992 ]\n",
      " [ 0.87754169 -0.04139728  0.4215278  ... -0.12759557 -0.32730142\n",
      "   0.63505085]\n",
      " [ 0.98795262 -0.25224437  0.86820121 ... -1.44767935  2.30604239\n",
      "  -0.3456529 ]\n",
      " ...\n",
      " [-1.22244576 -0.45272609 -1.74057708 ... -0.25257253  1.04105811\n",
      "   0.5675044 ]\n",
      " [-2.47316942  1.99770204  0.17841156 ...  1.40077656 -0.45673777\n",
      "   1.40450841]\n",
      " [ 0.04512419 -1.10929368  0.89201495 ... -0.13570963 -0.19962704\n",
      "  -1.91763652]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3407)\n",
    "R = np.random.normal(0, 1, (25, 384))\n",
    "\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fire lol'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(['fire', 'lol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://mosaic.scdn.co/640/ab67616d0000b2731bff3b5284c581b83e918d19ab67616d0000b273369abd4e740707a20132405eab67616d0000b27354dbcad8ae25bba3e552c537ab67616d0000b27391e6b5d4c6ffec6a50adda3f\n"
     ]
    }
   ],
   "source": [
    "playlist_url = 'https://open.spotify.com/playlist/2AXUGe93UP8oVj3F0a1Hoo?si=836ee38e473943a6'\n",
    "playlist = sp.playlist(playlist_url)\n",
    "playlist_image_url = playlist['images'][0]['url']\n",
    "print(playlist_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
